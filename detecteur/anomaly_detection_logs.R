#Get all the functions of the Entropy package.
source('/data/entropy.R')

#GENERAL USAGE

#1. Use iterate_logs to get the global count per hour for each doc

#2 Use get_anomalies_entropy to get each anomalies and the entropy value of each distribution (with "Laplace")


#Iteration through the list of logs retrieved by select_logs
iterate_logs <- function(path = getwd(), pattern="\\.log", start_date = "", end_date = "", type = "") {
  list_logs <- select_logs(path, pattern, start_date, end_date, type)
  listing_files = paste0(list_logs$file_names, collapse=", ")
  print(paste0("Downloading the following files : ", listing_files, collapse=" "))
  full <- ""
  file_names <- list_logs$file_names
  for (file in file_names) {
    data_logs <- cleanlog(file)
    full <- rbind(full, data_logs)
  }
  full <- get_count_hours(full)
  return(full)
}

#A function to select the relevant logs through several metadata: type ("REVUES", "HYPOTHESES"), start date and end date…
select_logs <- function(path = getwd(), pattern="\\.log", start_date = "", end_date = "", type = "") {
  out = ""
  file_names <- dir(path, pattern=pattern)
  file_date <- file_names %>% str_extract("\\d+") %>% mdy()
  file_type <- file_names %>% str_extract("\\w+\\.log$")
  str_sub(file_type, -4, -1) <- ""
  data_logs <- data_frame(file_names, file_date, file_type)
  if (type != "") {
    data_logs <- data_logs %>% filter(file_type == type)
  }
  if (start_date != "") {
    data_logs <- data_logs %>% filter(file_date >= ymd(start_date))
  }
  if (end_date != "") {
    data_logs <- data_logs %>% filter(file_date <= ymd(end_date))
  }
  return(data_logs)
}


#Optimized function to read logs: takes about 30 seconds per logs
cleanlog <- function(namelog) {
  logs <- read_log(namelog)
  colnames(logs) <- c("clientip", "V2", "V3", "timestamp", "doc", "status", "size", "refer", "agent")
  logs <- tbl_df(logs)
  logs <- logs[startsWith(logs$doc, "GET h"),] #Use startsWith rather than regex: takes less than a second.
  return(logs)
}

#A function to get the number of logs per hour
get_count_hours <- function(counts_revue) {
  match_revues = ".+?(?=\\.org/)" #Keep a regex for removing everything after .org : while it still takes 8 seconds all the other methods were much less effective (24 s for separate)
  counts_revue$doc <- str_extract(counts_revue$doc, match_org)
  str_sub(counts_revue$timestamp, -11, -1) <- "00:00"
  counts_revue$timestamp <- dmy_hms(counts_revue$timestamp) #Get date format
  counts_revue <- counts_revue %>% group_by(doc, timestamp) %>% summarise(count_consult=n()) %>% ungroup() #merge to each minute to get the freq counts
  pattern_to_match <- "http(.+?)https"
  counts_revue <- counts_revue[!str_detect(counts_revue$doc, "http(.+?)https"),]
  counts_revue <- counts_revue %>% filter(!is.na(doc)) %>% mutate(doc = as.factor(doc))
  return(counts_revue)
}


#A function to complete the time sequences (apparently zero logs are not always recorded) and remove Midnight.
complete_time <- function(count_hours) {
  time_sequence <- seq(count_hours$timestamp[which.min(count_hours$timestamp)], count_hours$timestamp[which.max(count_hours$timestamp)], by="hour") #First we get the complete sequence of "hours" during the period (by selection the min and max time recorded in timestamp)
  time_sequence <- data_frame(time_sequence, hour = hour(time_sequence)) %>% filter(hour != 0) #We extract the hour and remove "midnight" (which is highly irregularly recorded…)
  count_hours <- count_hours %>% group_by(doc) %>% complete(timestamp = time_sequence$time_sequence, fill = list(count_consult = 0))
  return(count_hours)
}


#A wrapper for AnomalyDetectionTs in order to make it compatible with do.
extract_anomalies <- function(timestamp, count_consult, doc, alpha_anom) {
  df <- data.frame(timestamp,count_consult)
  colnames(df) <- c("timestamp", "count_consult")
  res_revue = AnomalyDetectionTs(df, max_anoms=0.1, alpha =alpha_anom, direction='both', plot=FALSE)$anoms
  res_revue$timestamp <- as.character(res_revue$timestamp)
  res_revue$anoms <- as.character(res_revue$anoms)
  return(tbl_df(res_revue))
}


#A function to get the anomalies and the entropy
get_anomalies_entropy <- function(count_hours) {
  count_hours <- complete_time(count_hours)
  count_hours %<>% group_by(doc) %>%
    mutate(ifelse(count_consult == 0, 1, count_consult)) %>% #Pass default consultation number to 1 (to avoid any problem with subsequent calculations)
    do(timestamp_anoms = extract_anomalies(timestamp = .$timestamp, count_consult = .$count_consult, doc = .$doc, 0.05), entropy_rate = entropy(.$count_consult, method="Laplace")) %>% #use do to "loop" on each group of hour count per doc
    unnest(entropy_rate) %>% #unnest the list of entropy number (only contains one value)
    unnest(timestamp_anoms) %>% #unnest the dataframe generated by extract_anomalies.
    rename(anom_timestamp = timestamp, anom_log_count = anoms)
  return(count_hours)
}